{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff11582e-079b-48d4-9da9-71a7c0702c5f",
   "metadata": {},
   "source": [
    "# Amazon Bedrock boto3 Setup\n",
    "\n",
    "---\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the [`boto3` Python SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to work with [Amazon Bedrock](https://aws.amazon.com/bedrock/) Foundation Models.\n",
    "\n",
    "For more details refer to the [`Bedrock` Workshop](https://github.com/aws-samples/amazon-bedrock-workshop)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run the cells in this section to install the packages needed by the notebooks in this workshop. ⚠️ You will see pip dependency errors, you can safely ignore these errors. ⚠️\n",
    "\n",
    "IGNORE ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e558223",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n",
    "%pip install \"unstructured[csv]\" \"pillow>=9.5,<10\" langchain==0.0.309 python-weather pypdf==3.8.1 ipywidgets --force-reinstall --quiet\n",
    "%pip install /g/faiss-1.7.4/build/faiss/python\n",
    "%pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f06fbee0-9db9-4a9f-8792-b71a670d82f8",
   "metadata": {},
   "source": [
    "## Bedrock clients\n",
    "This notebook demonstrates invoking Bedrock models directly using the AWS SDK\n",
    "\n",
    "Here are links for some packages you might not be familiar with:\n",
    "\n",
    "- [FAISS](https://github.com/facebookresearch/faiss), to store vector embeddings\n",
    "- [sqlalchemy](https://www.sqlalchemy.org/), SQLAlchemy is the Python SQL toolkit \n",
    "- [PyPDF](https://pypi.org/project/pypdf/), for handling PDF files\n",
    "- [IPyWidgets](https://ipywidgets.readthedocs.io/en/stable/), for interactive UI widgets in the notebook\n",
    "- [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc483333",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the boto3 client\n",
    "\n",
    "Interaction with the Bedrock API is done via the AWS SDK for Python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html).\n",
    "\n",
    "Depending on your environment, you might need to customize the setup when creating your Bedrock service client. To help with this, we've provided a `get_bedrock_client()` utility method that supports passing in different options. You can find the implementation in [../utils/bedrock.py](../utils/bedrock.py)\n",
    "\n",
    "#### Use different clients\n",
    "The boto3 provides different clients for Amazon Bedrock to perform different actions. The actions for [`InvokeModel`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and [`InvokeModelWithResponseStream`](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) are supported by Amazon Bedrock Runtime where as other operations, such as [ListFoundationModels](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_ListFoundationModels.html), are handled via [Amazon Bedrock client](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_Operations_Amazon_Bedrock.html).\n",
    "\n",
    "The `get_bedrock_client()` method accepts `runtime` (default=True) parameter to return either `bedrock` or `bedrock-runtime` client.\n",
    "\n",
    "#### Use the default credential chain\n",
    "\n",
    "If you are running this notebook from [Amazon Sagemaker Studio](https://aws.amazon.com/sagemaker/studio/) and your Sagemaker Studio [execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has permissions to access Bedrock you can just run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock.\n",
    "\n",
    "#### Use a different AWS Region\n",
    "\n",
    "If you're running this notebook from your own computer or a SageMaker notebook in a different AWS Region from where Bedrock is set up, you can un-comment the `os.environ['AWS_DEFAULT_REGION']` line below and specify the region to use.\n",
    "\n",
    "#### Use a specific profile\n",
    "\n",
    "In case you're running this notebook from your own computer where you have setup the AWS CLI with multiple profiles, and the profile which has access to Bedrock is not the default one, you can un-comment the `os.environ['AWS_PROFILE']` line below and specify the profile to use.\n",
    "\n",
    "#### Use a different role\n",
    "\n",
    "In case you or your company has setup a specific, separate [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) to access Bedrock, you can specify it by un-commenting the `os.environ['BEDROCK_ASSUME_ROLE']` line below. Ensure that your current user or role have permissions to [assume](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) such role.\n",
    "\n",
    "#### A note about `langchain`\n",
    "\n",
    "LangChain is a framework designed to simplify the development of applications that leverage large language models (LLMs). It provides tools and abstractions to help developers build applications that can interact with LLMs, manage their outputs, and integrate them into larger workflows.\n",
    "\n",
    "Key features of LangChain include:\n",
    "\n",
    "- **LLM Wrappers:** Simplified interfaces to interact with various LLMs.\n",
    "- **Chains:** Sequences of calls to LLMs or other utilities, allowing for complex workflows.\n",
    "- **Memory:** Mechanisms to maintain state between calls to LLMs.\n",
    "- **Agents:** Components that use LLMs to make decisions and take actions.\n",
    "- **Data Connectors:** Integrations with various data sources and sinks.\n",
    "\n",
    "LangChain is particularly useful for building applications like chatbots, automated content generation tools, and other AI-driven applications that require sophisticated language understanding and generation capabilities.\n",
    "\n",
    "The Bedrock classes provided by `langchain` create a Bedrock boto3 client by default. To customize your Bedrock configuration, we recommend to explicitly create the Bedrock client using the method below, and pass it to the [`langchain.Bedrock`](https://python.langchain.com/docs/integrations/llms/bedrock) class instantiation method using `client=boto3_bedrock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628ef76-b545-4bf9-ade6-5b9f5c2c62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock #, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    # The key \"temperature\" is set to 0.0. In the context of NLP models,\n",
    "    # temperature is a parameter that controls the randomness of predictions\n",
    "    # by scaling the logits before applying the softmax function. A lower\n",
    "    # temperature (close to 0) makes the model more deterministic, producing\n",
    "    # less random and more focused outputs. Conversely, a higher temperature\n",
    "    # results in more random and diverse outputs.\n",
    "    \"temperature\": 0.0,\n",
    "    # The key \"top_p\" is set to 0.5. This parameter is used in nucleus\n",
    "    # sampling, a technique for generating text. The top-p value determines\n",
    "    # the cumulative probability threshold for selecting the next token. When\n",
    "    # generating text, the model considers only the smallest set of tokens\n",
    "    # whose cumulative probability is at least top_p. In this case, setting\n",
    "    # top_p to 0.5 means the model will consider tokens that together have a\n",
    "    # 50% probability of being the next token, promoting more coherent and\n",
    "    # contextually relevant outputs.\n",
    "    \"top_p\": .5,\n",
    "    # The key \"max_tokens_to_sample\" is set to 2000. This parameter specifies\n",
    "    # the maximum number of tokens the model is allowed to generate in a single\n",
    "    # sampling session. Limiting the number of tokens helps control the length\n",
    "    # of the generated text, ensuring it does not exceed a certain size, which\n",
    "    # can be useful for managing computational resources and maintaining\n",
    "    # relevance in the generated content.\n",
    "    \"max_tokens_to_sample\": 2000\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2a627a-5b23-429d-9ee2-12130eb98d01",
   "metadata": {},
   "source": [
    "## Invoke Model\n",
    "\n",
    "we will demonstrate the use of invoking the models and also highlight the default behaviour of LLM. They will return answer and data based on the training data. For our use case of AiStylist we need more curated responses. That means we will need to adopt a different architecture to get the model to execute our use case. Those techniques will be demonstrated in the next workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad4159-24aa-486d-8872-328109f0f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "# customer_input = (\n",
    "#     \"I am a male consultant in my 30s traveling to New York next week. \"\n",
    "#     \"What kind of outfit should I wear on my first day in the office? \"\n",
    "# )\n",
    "\n",
    "customer_input = (\n",
    "    \"I work in my basement in gym clothes. I need to travel to a client \"\n",
    "    \"location in a nearby city. What kind of outfit should I wear on my first \"\n",
    "    \"day in the office? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0fcf2-2a2e-4e30-9085-10dbd2967895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first fetch possible styles and give options to customer\n",
    "# the phrase \"list different styles\" tells the model to output a list\n",
    "# which will be parsed, and important part of using Amazon Bedrock and\n",
    "# LLMs is prompt engineering\n",
    "# https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\n",
    "prompt = \"\"\"\n",
    "\n",
    "Human: list different style options for:\n",
    "\"\"\" + customer_input + \"\"\"\n",
    "\n",
    "Assistant:\"\"\"\n",
    "body = json.dumps({\"prompt\": prompt, \"max_tokens_to_sample\": 500})\n",
    "\n",
    "# change this to use a different version from the model provider\n",
    "# show the available models in the AWS console\n",
    "model_id = \"anthropic.claude-v2\"\n",
    "accept = \"application/json\"\n",
    "content_type = \"application/json\"\n",
    "\n",
    "response = bedrock_runtime.invoke_model(\n",
    "    body=body,\n",
    "    modelId=model_id,\n",
    "    accept=accept,\n",
    "    contentType=content_type\n",
    ")\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "styles_response = response_body.get(\"completion\")\n",
    "print(styles_response)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a3c215a-b09c-486b-b73a-e8e2adb824ad",
   "metadata": {},
   "source": [
    "## Generate the default images using stability.stable-diffusion-xl\n",
    "\n",
    "Preparing received response to generate images for each style look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049336b-f638-483c-bb8a-cc3d72c2aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for fetching images for each of style\n",
    "styles = [\n",
    "    s.strip() for s in (list(filter(None, styles_response.splitlines()))[1:-1])\n",
    "]\n",
    "print(*styles, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d5c8-00da-41f1-83fa-8e84c4e415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from base64 import b64decode\n",
    "import json\n",
    "\n",
    "# Fetching images for each of style\n",
    "\n",
    "images = []\n",
    "for i, style in enumerate(styles):\n",
    "    if len(style.split(\"-\")) > 1:\n",
    "\n",
    "        # https://platform.stability.ai/docs/api-reference#tag/SDXL-1.0-and-SD1.6/operation/textToImage\n",
    "        # text_prompts: A list containing a dictionary with:\n",
    "        # text: A string created by concatenating \"person wearing \" with the\n",
    "        #       second part of the style variable (split by \"-\").\n",
    "        # weight: A float value (1.0) indicating the importance or influence\n",
    "        #         of this text prompt.\n",
    "        # cfg_scale: A parameter (set to 5) that likely controls the strength\n",
    "        #            of adherence to the text prompt.\n",
    "        # seed: A fixed integer (5450) used to initialize the random number\n",
    "        #       generator for reproducibility.\n",
    "        # steps: An integer (70) specifying the number of steps for the\n",
    "        #        generation process, which might affect the quality or detail\n",
    "        #        of the output.\n",
    "        # style_preset: A string (\"photographic\") indicating the style or\n",
    "        #               preset to be applied to the generated output.\n",
    "        request = json.dumps({\n",
    "            \"text_prompts\": (\n",
    "                [{\n",
    "                    \"text\": \"person wearing \" + style.split(\"-\")[1],\n",
    "                    \"weight\": 1.0\n",
    "                }]\n",
    "            ),\n",
    "            \"cfg_scale\": 5,\n",
    "            \"seed\": 5450,\n",
    "            \"steps\": 70,\n",
    "            \"style_preset\": \"photographic\",\n",
    "        })\n",
    "        model_id = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "        response = bedrock_runtime.invoke_model(body=request, modelId=model_id)\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "        base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "        display.display(display.Image(b64decode(base_64_img_str), width=200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
