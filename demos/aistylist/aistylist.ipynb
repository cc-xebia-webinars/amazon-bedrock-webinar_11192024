{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff11582e-079b-48d4-9da9-71a7c0702c5f",
   "metadata": {},
   "source": [
    "![Embeddings](./images/AIStylist.png)\n",
    "# AI Stylist - Conversational Shopping Experience with Amazon Bedrock\n",
    "\n",
    "\n",
    "In this notebook, we will build an AI virtual assistant acting as your stylist. You will learn how to build an application that help users choose clothes from a product catalog, all powered by Large Language Models (LLMs). We will leverage Amazon Bedrock for using the Foundation Models (FMs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fd58d9a",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Virtual Assistants are becoming increasingly popular as LLMs start to power the interaction with users. These applications demand an ability to `understand` the intent of the user input and generate adaptable sequence of calls to language models and various utilities depending on user input.\n",
    "\n",
    "### Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience of the customers. Virtual assistants use natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We can improve upon this process by implementing an architecture called `Retreival Augmented Generation (RAG)`. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. Virtual assistants need ability to determine the sequence of events needed to generate the final results and as such we build towards the use, `plan-and-execute` agents along with `Zero-shot ReAct` which is an action agent and uses [`ReAct`](https://arxiv.org/pdf/2205.00445.pdf) pattern to select the appropriate calls. These will then develop into `Agents` concept. The Agent interface enables such flexibility for these applications.\n",
    "\n",
    "There are two primary categories of agents:\n",
    "\n",
    "- Action agents: At each interval, determine the subsequent action utilizing the outputs of all previous actions. \n",
    "- Plan-and-execute agents: Determine the complete order of actions initially, then implement them all without updating the plan.\n",
    "\n",
    "### Challenges\n",
    "- Parse the user query and extracting information which can be used to search the catalog\n",
    "- Reform the prompts to bring products from the catalog\n",
    "- Search product catalog to bring relevant products with details like product id, description etc\n",
    "- Generate relevant images for a style based on the user chat\n",
    "- Continue the shopping experience workflow for adding products or finalizing the sale\n",
    "- Identify relevant products based on user location or questions asked by user\n",
    "\n",
    "### Proposal\n",
    "\n",
    "To the above challenges, this notebook proposes the following strategy\n",
    "\n",
    "#### Prepare documents\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the product catalog must be processed and a stored in a document store index\n",
    "- Load the product catalog and specifically the descriptions\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "\n",
    "#### Catalog operations. Fetch relevant products from the catalog\n",
    "\n",
    "We need to leverage the LLM for the following \n",
    "- Extract information of the `intent` and other details like `place` , objective of the purchase\n",
    "- Use [LangChain](https://python.langchain.com/docs/get_started/introduction) as orchestrator to do a similarity search from the vector store and bring back the artifacts matching the query\n",
    "- Extract information for `Product ID` from the search results  and conduct a search to bring further details of the selected products\n",
    "- Leverage [LangChain](https://python.langchain.com/docs/get_started/introduction) to orchestrate and call API's using the `ReAct` framework to bring other details like weather and conduct search to bring back the relevant products\n",
    "\n",
    "#### Query\n",
    "\n",
    "- Identify the `intent` of the query\n",
    "- Orchestrate the workflow to move the users to the next steps in their purchase journey\n",
    "\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "When the document's index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model under Amazon Bedrock\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1fd7f92",
   "metadata": {},
   "source": [
    "\n",
    "## Virtual Assistant using Amazon Bedrock\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/context-aware-chatbot.png)\n",
    "\n",
    "## Langchain framework for building Virtual Assistants with Amazon Bedrock\n",
    "In Conversational interfaces such as virtual assistants, it is highly important to remember previous interactions, both at a short term but also at a long term level.\n",
    "\n",
    "LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains.\n",
    "It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.\n",
    "\n",
    "## Building Chatbot with Context - Key Elements\n",
    "\n",
    "The first process in a building a contextual-aware chatbot is to **generate embeddings** for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using Titan Embeddings model for this\n",
    "\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Second process is the user request orchestration , interaction,  invoking and returning the results\n",
    "\n",
    "![Chatbot](./images/chatbot_lang.png)\n",
    "\n",
    "## Architecture\n",
    "![Architecture](./images/Architecture.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "873f657c",
   "metadata": {},
   "source": [
    "### <font color='red'>Setup</font> \n",
    "---\n",
    "<font color='red'>⚠️ ⚠️ ⚠️</font> \n",
    "Before running this notebook, ensure you've run the [Intro to Bedrock notebook](./intro_to_bedrock.ipynb) notebook. <font color='red'>⚠️ ⚠️ ⚠️</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f06fbee0-9db9-4a9f-8792-b71a670d82f8",
   "metadata": {},
   "source": [
    "## Configure Bedrock\n",
    "\n",
    "Create the necessary clients to invoke Bedrock models. If you would need to pass in a certain role then set those values appropriately\n",
    "We begin with instantiating the LLM and the Embeddings model. Here we are using Anthropic Claude V2 for text generation and Titan Embeddings G1 - Text for text embeddings.\n",
    "\n",
    "Note: It is possible to choose other models available with Bedrock. You can replace the `model_id` as follows to change the model.\n",
    "\n",
    "`llm = Bedrock(model_id=\"anthropic.claude-v2\")`\n",
    "\n",
    "You can read about all the available model IDs [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids-arns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5873db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2628ef76-b545-4bf9-ade6-5b9f5c2c62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from IPython.display import Image\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "# admin client\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=False)\n",
    "\n",
    "# model execution client\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n",
    "\n",
    "model_parameter = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": .5,\n",
    "    \"top_k\": 250,\n",
    "    \"max_tokens_to_sample\": 2000,\n",
    "    \"stop_sequences\": [\"\\n\\n Human: bye\"]\n",
    "}\n",
    "\n",
    "# configure the LLM for reuse\n",
    "llm = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs=model_parameter,\n",
    "    client=bedrock_runtime\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1473baff",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "In order to follow the approach this notebook is using the LangChain framework where it has integrations with different services and tools that allow efficient building of patterns such as RAG. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: Anthropic Claude V2 available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to understand the document chunks and provide an answer in human friendly manner.\n",
    "- **Embeddings Model**: Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This model will be used to generate a numerical representation of the textual documents\n",
    "- **Document Loader**: [S3FileLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.s3_file.S3FileLoader.html) and PDF Loader available through LangChain\n",
    "\n",
    "  This is the loader that can load the documents from a source, for the sake of this notebook we are loading the sample files from a local path. This could easily be replaced with a loader to load documents from enterprise internal systems.\n",
    "\n",
    "- **Vector Store**: In-Memory store FAISS\n",
    "\n",
    "  The index helps to compare the input embedding and the document embeddings to find relevant document\n",
    "- **Wrapper**: wraps index, vector store, embeddings model and the LLM to abstract away the logic from the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2a627a-5b23-429d-9ee2-12130eb98d01",
   "metadata": {},
   "source": [
    "## Simulate a user ask of the AI Stylist \n",
    "\n",
    "![Generate Look](./images/user_prompt.png)\n",
    "\n",
    "### Try these prompts if you would like\n",
    "- `\"I am a female journalist in my 30s traveling to New York next week. What kind of outfit should I wear on my first day at New York Times?\"`\n",
    "- `\"I am male athlete looking for suitable outfit for attending half marathon in San Fransisco.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad4159-24aa-486d-8872-328109f0f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to try your own prompt, edit this parameter!\n",
    "# customer_input = (\n",
    "#     \"I am a male consultant in my 30s traveling to New York next week. \"\n",
    "#     \"What kind of outfit should I wear on my first day in the office?\"\n",
    "# )\n",
    "\n",
    "customer_input = (\n",
    "    \"I am a young woman in early 20's traveling to Norway in January. \"\n",
    "    \"What kind of outfit should I wear to view the northern lights north \"\n",
    "    \"of the arctic circle?\"\n",
    ")\n",
    "\n",
    "\n",
    "# Customer id to infuse order history and delivery address\n",
    "customer_id = \"2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "686b8bcf-3a36-4fbf-aa55-2a5c4ea15d90",
   "metadata": {},
   "source": [
    "## Extract `product relevant` information\n",
    "\n",
    "We have our product catalog stored in a data base and the key attributes for those are \n",
    "1. Product type\n",
    "2. Age group for the product\n",
    "3. Season for use of the product\n",
    "4. Description of the product\n",
    "5. Images for the product\n",
    "\n",
    "We will instruct LLM to fetch the relevant information from the user prompt based on the above so we can query our catalog and bring back relevant results. Note we have a very specific prompt template for the `entity extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4210f-f08b-49d2-b9b7-45da05657fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify product attributes from customer prompt to generate better results\n",
    "\n",
    "# The prefix \"ner\" stands for Named Entity Recognition. It is a subtask of\n",
    "# information extraction in natural language processing (NLP) that involves\n",
    "# identifying and classifying named entities mentioned in text into predefined\n",
    "# categories such as the names of persons, organizations, locations, dates,\n",
    "# and other entities.\n",
    "\n",
    "# For example, in the sentence \"Barack Obama was born in Hawaii,\" NER would\n",
    "# identify \"Barack Obama\" as a person and \"Hawaii\" as a location.\n",
    "\n",
    "ner_prompt = \"\"\"Human: Find person age group, gender, season and the location in the customer input.\n",
    "Instructions:\n",
    "The age group can be one of the following: 10-20, 20-30, 30-50, 50+\n",
    "The gender can be one of the following: Mens, Womens, Other\n",
    "The gender can also be derived from the name if not explicitly mentioned\n",
    "The season can be one of the following: summer, winter, spring, fall\n",
    "The output must be in JSON format inside the tags <attributes></attributes>\n",
    "Today's date is June 1st.\n",
    "\n",
    "If the information of an entity is not available in the input then don't include that entity in the JSON output\n",
    "\n",
    "Begin!\n",
    "\n",
    "Customer input: {customer_input}\n",
    "Assistant:\"\"\"\n",
    "entity_extraction_result = llm(\n",
    "    ner_prompt.format(customer_input=customer_input)).strip()\n",
    "print(entity_extraction_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaddb1cd",
   "metadata": {},
   "source": [
    "#### Extract values into JSON\n",
    "\n",
    "Since we have instructed LLM to return our data as XML wrapping a JSON, we run the necessary extraction steps to fetch the relevant details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "result = re.search(\n",
    "    '<attributes>(.*)</attributes>',\n",
    "    entity_extraction_result,\n",
    "    re.DOTALL)\n",
    "attributes = json.loads(result.group(1))\n",
    "attributes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c556d2c-d133-4bce-a4a2-27721191f8bb",
   "metadata": {},
   "source": [
    "## Use Retrieval Augmented Generation (RAG) \n",
    "\n",
    "We will leverage the semantic search to embed product-accessory catalogs and order history for the embeddings created by `Amazon Titan Embeddings Text v1`\n",
    "\n",
    "After downloading we can load the documents with the help of [S3FileLoader available under LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/) and splitting them into smaller chunks.\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt. Also the embeddings model has a limit of the length of input tokens limited to 8k tokens, which roughly translates to ~32000 characters. For the sake of this use-case we are creating chunks of roughly 1000 characters with an overlap of 100 characters using [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html).\n",
    "\n",
    "Here we are fetching our product catalog and creating the embeddings for \n",
    "1. Customer reviews\n",
    "2. Order History\n",
    "3. Product catalog description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0fcf2-2a2e-4e30-9085-10dbd2967895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the Titan Embeddings Model to generate our Embeddings.\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import S3FileLoader\n",
    "import numpy as np\n",
    "\n",
    "# - create the Titan Embeddings Model\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\",\n",
    "                                       client=bedrock_runtime)\n",
    "customer_reviews_loader = S3FileLoader(\n",
    "    \"sagemaker-example-files-prod-us-east-1\",\n",
    "    \"datasets/image/howser-bedrock/data/aistylist/data/customer_reviews.csv\",\n",
    "    region_name=\"us-east-1\")\n",
    "order_history_loader = S3FileLoader(\n",
    "    \"sagemaker-example-files-prod-us-east-1\",\n",
    "    \"datasets/image/howser-bedrock/data/aistylist/data/order_history.csv\",\n",
    "    region_name=\"us-east-1\")\n",
    "products_catalog_loader = S3FileLoader(\n",
    "    \"sagemaker-example-files-prod-us-east-1\",\n",
    "    \"datasets/image/howser-bedrock/data/aistylist/data/products_catalog.csv\",\n",
    "    region_name=\"us-east-1\")\n",
    "\n",
    "documents = []\n",
    "customer_reviews = customer_reviews_loader.load()\n",
    "order_history = order_history_loader.load()\n",
    "products_catalog = products_catalog_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, to test an alternate behavior.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "\n",
    "# Embedding data here. Add more data for better results\n",
    "documents += text_splitter.split_documents(customer_reviews)\n",
    "documents += text_splitter.split_documents(order_history)\n",
    "documents += text_splitter.split_documents(products_catalog)\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    documents, # Documents to embed\n",
    "    bedrock_embeddings, # Embedding model\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "\n",
    "# create an embedding for the customer input which is then used to query the\n",
    "# index\n",
    "query_embedding = vectorstore_faiss.embedding_function(customer_input)\n",
    "np.array(query_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1fd6c9d-a512-4f1e-ba8f-486d8b88f71f",
   "metadata": {},
   "source": [
    "## Generate *`n`* style recommendations\n",
    "\n",
    "Make a query to embed the LLM using customer input. Using LangChain for orchestration of RAG. It also provides a framework for orchestrating RAG flows with what purpose built \"chains\". In this section, we will see how to be a [retrieval chain](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa) which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process:\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve the relevant products\n",
    "3. Product description and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. This output is fed into the stable diffusion model to return the relevant images\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and product description. \n",
    "3. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "4. Reformatted user input is queried against the vector database to retrieve relevant products.The reformatted user input and relevant documents are inserted into a new prompt in order to generate the new style. \n",
    "5. This is then fed into the stable diffusion model to generate the images. \n",
    "\n",
    "In your application the images can come from a pre canned images \n",
    "\n",
    "We will now build out this second workflow using LangChain below. First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff0c14-5bfb-48e6-a63d-0b949f655b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to generate 5 style recommendations for the customer input at the end.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<example>A navy suit with a light blue dress shirt, conservative tie, black oxford shoes, and a leather belt.</example>\n",
    "<example>A lehenga choli set with a crop top, flowing skirt, and dupatta scarf in lively colors and metallic accents.</example>\n",
    "\n",
    "Customer Input: {question}\n",
    "Each style recommendation must be inside the tags <style></style>.\n",
    "Do not output product physical IDs.\n",
    "Skip the preamble.\n",
    "Assistant: \"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Use RetrievalQA customizations for improving Q&A experience\n",
    "# qa = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=vectorstore_faiss.as_retriever(\n",
    "#         search_type=\"similarity\", search_kwargs={\"k\": 6}\n",
    "#     ),\n",
    "#     return_source_documents=False,\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT},\n",
    "# )\n",
    "# styles_response = qa({\"query\": customer_input})['result']\n",
    "\n",
    "# Alternatively we can query using wrapper also\n",
    "styles_response = wrapper_store_faiss.query(question= customer_input, llm=llm)\n",
    "\n",
    "print_ww(styles_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb57741b-8724-4504-9a7f-3be6c9ddc2c9",
   "metadata": {},
   "source": [
    "### Prepare the received response\n",
    "\n",
    "Since we have instructed LLM to return our data is returned as XML wrapping a JSON, we run the necessary extraction steps to fetch the relevant details to generate images for each look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6b755-b54f-401e-8742-ce7dc1757ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input to fetch images for each look\n",
    "# styles = re.findall('<style>(.*?)</style>', styles_response)\n",
    "\n",
    "# uncomment when using the wrapper\n",
    "styles = [\n",
    "    style[2:]\n",
    "    for style in styles_response.splitlines()[1:-1]\n",
    "    if len(style) > 0]\n",
    "styles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d22783-c906-413e-a889-1c4d3dd1f0fe",
   "metadata": {},
   "source": [
    "## Generate Images for the relevant style\n",
    "\n",
    "Generate an image for each look using the `Stable Diffusion` model\n",
    "\n",
    "![Generate Look](./images/generate_look.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7e6ff-7e46-403b-9c38-f98de88bdeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import json\n",
    "\n",
    "# Fetching images for each of style\n",
    "gender_map = {\n",
    "    'Womens': 'of a female ',\n",
    "    'Mens': 'of a male '\n",
    "}\n",
    "\n",
    "image_strip = \"\"\n",
    "for i, style in enumerate(styles):\n",
    "    gender_str = gender_map.get(attributes.get('gender'))\n",
    "    request = json.dumps({\n",
    "        \"text_prompts\": [\n",
    "            {\n",
    "                \"text\": (\n",
    "                    f\"Full body view {gender_str}\"\n",
    "                    \"without a face in \" + style + \"dslr, ultra quality, \"\n",
    "                    \"dof, film grain, Fujifilm XT3, crystal clear, 8K UHD\"\n",
    "                ),\n",
    "                \"weight\": 1.0\n",
    "            },\n",
    "            {\"text\": \"poorly rendered\", \"weight\": -1.0}\n",
    "        ],\n",
    "        \"cfg_scale\": 9,\n",
    "        \"seed\": 4000,\n",
    "        \"steps\": 50,\n",
    "        \"style_preset\": \"photographic\",\n",
    "    })\n",
    "    model_id = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(body=request, modelId=model_id)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "\n",
    "    # this will display the images in a vertical column\n",
    "    # display.display(display.Image(b64decode(base_64_img_str), width=200))\n",
    "\n",
    "    # this will display the images in a horizontal strip\n",
    "    image_strip += (\n",
    "        \"<td><img src='data:image/png;base64, \"\n",
    "        f\"{base_64_img_str}\"\n",
    "        \"'></td>\"\n",
    "    )\n",
    "\n",
    "# display the image strip in a HTML table\n",
    "display.display(display.HTML(\"<table><tr>\" + image_strip +\"</tr></table>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e0443f-487f-4c49-87c2-ab581be8535f",
   "metadata": {},
   "source": [
    "## Enhance user experience with Chatbot\n",
    "\n",
    "#### Generating detailed overview based on customer reviews of products in catalog \n",
    "We have discussed the key building blocks needed for the chatbot application and now we will start to create them. LangChain's [ConversationBufferMemory](https://python.langchain.com/docs/use_cases/question_answering/chat_vector_db) class provides an easy way to capture conversational memory for LLM chat applications. We will have Claude being able to retrieve context through conversational memory using the prompt template. Note that this time our prompt template includes a {chat_history} variable where our chat history will be included to the prompt.\n",
    "\n",
    "The prompt template has both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual.\n",
    "\n",
    "\n",
    "![Architecture](./images/chatbot_products.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e05cd-da0e-4649-95e9-59e2cc9358dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt1 = \"Show me specific reviews that talk about the quality of the fabric for the jacket.\"\n",
    "chat_prompt2 = \"What do people like about the jacket?\"\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "print(CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "chat_history = [\" \"]\n",
    "memory_chain = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True)\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore_faiss.as_retriever(),\n",
    "    memory=memory_chain,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    #verbose=True,\n",
    "    chain_type='stuff', # 'refine',\n",
    "    #max_tokens_limit=300\n",
    ")\n",
    "\n",
    "# Generate detailed reviews based on customer reviews of specific clothing\n",
    "# in product catalog\n",
    "\n",
    "try:\n",
    "    chat_res1 = conversation.run({\n",
    "        'question': chat_prompt1, 'chat_history': chat_history })\n",
    "    print_ww(chat_res1)\n",
    "    chat_history.append([chat_prompt1, chat_res1])\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        class StopExecutionError(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecutionError\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf8288-acbf-464e-b202-174755e7568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    chat_res2 = conversation.run({\n",
    "        \"question\": (\n",
    "            chat_prompt2\n",
    "        ),\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    print_ww(chat_res2)\n",
    "    chat_history.append([chat_prompt2, chat_res2])\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        class StopExecutionError(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecutionError\n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c0a1cb-72aa-4d26-bc17-d5846a1e4188",
   "metadata": {},
   "source": [
    "#### Customer order history semantic searches\n",
    "\n",
    "Generating size and color recommendations based on customer's order history. This will help to provide curated content to the customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4710f-c185-4356-a850-f2c051cf1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt3 = \"What size and color should I wear?\"\n",
    "chat_res3 = wrapper_store_faiss.query(\n",
    "    question= (\n",
    "        f\"{chat_prompt3} based on order history for customer with id \"\n",
    "        f\"{customer_id}\"\n",
    "    ), llm=llm)\n",
    "print_ww(chat_res3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b358e8-fa6b-4cbf-b05b-ea985dcfa260",
   "metadata": {},
   "source": [
    "## Showing final products based on customer style selection \n",
    "\n",
    "Continuing on our architectural pattern we will change the prompt template and leverage the LLM to generate the `recommended` products based on the user selection and weather and other details. The key extraction entities will be \n",
    "\n",
    "1. Leverage the customer initial prompt to generate the relevant ids\n",
    "2. Extract the relevant products from the vector store\n",
    "3. Physical ID for the products needed\n",
    "\n",
    "\n",
    "\n",
    "![Architecture](./images/other_products.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd18a21-5519-4543-8d8f-98ddab109b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template2 = \"\"\"Human: Extract list of products and their respective physical IDs from catalog that matches the style given below. \n",
    "The catalog of products is provided under <catalog></catalog> tags below.\n",
    "<catalog>\n",
    "{context}\n",
    "</catalog>\n",
    "Style: {question}\n",
    "\n",
    "The output should be a JSON of the form <products>[{{\"product\": <description of the product from the catalog>, \"physical_id\":<physical id of the product from the catalog>}}, ...]</products>\n",
    "Skip the preamble.\n",
    "Assistant: \"\"\"\n",
    "\n",
    "PROMPT2 = PromptTemplate(\n",
    "    template=prompt_template2, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "qa2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore_faiss.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 10}\n",
    "    ),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT2},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "selected_style = styles[3]\n",
    "print(selected_style)\n",
    "cart_items = qa2({\"query\": selected_style })['result']\n",
    "print_ww(cart_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2343e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = json.loads(re.findall('<products>(.*?)</products>', cart_items, re.DOTALL)[0])\n",
    "products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cd743-d2d6-4084-8113-68aa2fba8759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import urllib.parse\n",
    "\n",
    "cart_item_strip = \"\"\n",
    "for product in products:\n",
    "    url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(product['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "    # im = Image.open(requests.get(url, stream=True).raw)\n",
    "    cart_item_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "display.display(display.HTML(\"<table><tr>\" + cart_item_strip +\"</tr></table>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a073284-c491-4265-82a5-c48db9ea7a93",
   "metadata": {},
   "source": [
    "## Integrating DIY Agents to associate external APIs and databases\n",
    "### Using ReAct: Synergizing Reasoning and Acting in Language Models Framework\n",
    "Large language models can generate both explanations for their reasoning and task-specific responses in an alternating fashion. \n",
    "\n",
    "Producing reasoning explanations enables the model to infer, monitor, and revise action plans, and even handle unexpected scenarios. The action step allows the model to interface with and obtain information from external sources such as knowledge bases or environments.\n",
    "\n",
    "The ReAct framework could enable large language models to interact with external tools to obtain additional information that results in more accurate and fact-based responses. Here we will leverage the user prompt and perform the following actions\n",
    "1. Extract the city \n",
    "2. Get weather information\n",
    "3. Search our product catalog using semantic search to find relevant products\n",
    "4. Display the products for user to add to cart\n",
    "\n",
    "![Architecture](./images/weather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d6499-8a26-446f-9c6d-b518dc7edcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_weather\n",
    "\n",
    "async def getweather(city):\n",
    "  # declare the client. the measuring unit used defaults to the metric system (celcius, km/h, etc.)\n",
    "  async with python_weather.Client(unit=python_weather.IMPERIAL) as client:\n",
    "    # fetch a weather forecast from a city\n",
    "    weather = await client.get(city)\n",
    "\n",
    "    # returns the current day's forecast temperature (int)\n",
    "    return weather"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122aacf9-84e2-4aba-968a-6a07f6f98966",
   "metadata": {},
   "source": [
    "## Accessory recommendations \n",
    "\n",
    "We will provide  accessory recommendations based on location provided in customer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "await getweather(entity_extraction_result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204770ed-f303-4ee0-a258-21f87596e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessory_response = None\n",
    "if attributes[\"location\"]:\n",
    "    current_weather = await getweather(entity_extraction_result[2])\n",
    "    accessory_input = \"Suggest list of accessories based on the weather and the selected style. It is \" + current_weather.description + \" with temperature at \" + str(current_weather.temperature) +\" degrees fahrenheit.\\n Selected Style: \" + styles[0]\n",
    "    accessory_response = qa({\"query\": accessory_input})['result']\n",
    "    print_ww(accessory_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if accessory_response:\n",
    "    accessories = re.findall('<style>(.*?)</style>', accessory_response)\n",
    "    accessories_items = qa2({\"query\": ', '.join(accessories)})['result']\n",
    "    accessories_items = json.loads(re.findall('<products>(.*?)</products>', accessories_items, re.DOTALL)[0])\n",
    "    accessory_strip = \"\"\n",
    "    for accessory in accessories_items:\n",
    "        url = \"https://sagemaker-example-files-prod-us-east-1.s3.us-east-1.amazonaws.com/datasets/image/howser-bedrock/data/aistylist/images/products/\" + urllib.parse.quote(accessory['physical_id'].strip(), safe='', encoding=None, errors=None) + \".jpg\"\n",
    "        accessory_strip += \"<td><img src='\"+ url + \"'></td>\"\n",
    "    display.display(display.HTML(\"<table><tr>\" + accessory_strip +\"</tr></table>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4e689c0-8862-41a1-8867-e2bb10c2f64f",
   "metadata": {},
   "source": [
    "## Simulate the order check out\n",
    "\n",
    "Add a customer data table to complete the order transaction. This information provides the shipping address for the outfit order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f2e20-0c64-44fa-ab3b-8d2401a00c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_table=[{\"id\": 1, \"first_name\": \"John\", \"last_name\": \"Doe\", \"age\": 35, \"address\": \"123 Bedrock st, California 90210\"},\n",
    "  {\"id\": 2, \"first_name\": \"Jane\", \"last_name\": \"Smith\", \"age\": 27, \"address\": \"234 Sagemaker drive, Texas 12345\"},\n",
    "  {\"id\": 3, \"first_name\": \"Bob\", \"last_name\": \"Jones\", \"age\": 42, \"address\": \"111 DeepRacer ct, Virginia 55555\"},\n",
    "  {\"id\": 4, \"first_name\": \"Sara\", \"last_name\": \"Miller\", \"age\": 29, \"address\": \"222 Robomaker ave, New Yotk 13579\"},\n",
    "  {\"id\": 5, \"first_name\": \"Mark\", \"last_name\": \"Davis\", \"age\": 31, \"address\": \"444 Transcribe blvd, Florida 02468\"},\n",
    "  {\"id\": 6, \"first_name\": \"Laura\", \"last_name\": \"Wilson\", \"age\": 24, \"address\": \"555 CodeGuru st, California 98765\" },\n",
    "  {\"id\": 7, \"first_name\": \"Steve\", \"last_name\": \"Moore\", \"age\": 36, \"address\": \"456 DeepLens st, Texas 11223\"},\n",
    "  {\"id\": 8, \"first_name\": \"Michelle\", \"last_name\": \"Chen\", \"age\": 22, \"address\": \"642 DeepCompose st, Colorado 33215\"},\n",
    "  {\"id\": 9, \"first_name\": \"David\", \"last_name\": \"Lee\", \"age\": 29, \"address\": \"777 S3 st, California 99567\"},\n",
    "  {\"id\": 10, \"first_name\": \"Jessica\", \"last_name\": \"Brown\", \"age\": 18, \"address\": \"909 Ec st, Utah 43210\"}]\n",
    "\n",
    "def address_lookup(id):\n",
    "    for customer in customer_table:\n",
    "        if customer[\"id\"] == int(id):\n",
    "            return customer\n",
    "\n",
    "    return None\n",
    "\n",
    "print(address_lookup(customer_id)[\"address\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22dcfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
